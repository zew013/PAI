{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c25854-ccc3-41b2-aba3-0f6b7ed22689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f47b2-6214-4424-9556-215639e737ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e84dda0-b6a0-4651-86f8-5907a13049b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALAN\\anaconda3\\envs\\pai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ALAN\\anaconda3\\envs\\pai\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from attention import llama_modify\n",
    "from constants import INSTRUCTION_TEMPLATE, POPE_CHAT_PATH, SYSTEM_MESSAGE\n",
    "from eval_data_loader import POPEChatDataSet\n",
    "from llava.utils import disable_torch_init\n",
    "from model_loader import ModelLoader\n",
    "from tqdm import tqdm\n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "\n",
    "\n",
    "def setup_seeds():\n",
    "    seed = 927\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"POPE evaluation on LVLMs.\")\n",
    "parser.add_argument(\"--model\", type=str, help=\"model\")\n",
    "parser.add_argument(\"--pope-type\", type=str, help=\"model\")\n",
    "parser.add_argument(\n",
    "    \"--options\",\n",
    "    nargs=\"+\",\n",
    "    help=\"override some settings in the used config, the key-value pair \"\n",
    "    \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "    \"change to --cfg-options instead.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data-path\",\n",
    "    type=str,\n",
    "    default=\"/path/to/coco/val2014/\",\n",
    "    help=\"data path\",\n",
    ")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--beam\", type=int, default=1)\n",
    "parser.add_argument(\"--sample\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-attn\", action=\"store_true\")\n",
    "parser.add_argument(\"--alpha\", type=float, default=0.2)\n",
    "parser.add_argument(\"--use-mask\", action=\"store_true\")\n",
    "parser.add_argument(\"--use-cfg\", action=\"store_true\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=2)\n",
    "parser.add_argument(\"--start-layer\", type=int, default=2)\n",
    "parser.add_argument(\"--end-layer\", type=int, default=32)\n",
    "parser.add_argument(\"--max-tokens\", type=int, default=512)\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27e814c-0c72-4a01-a64a-9738e967d084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(model='llava-1.5', pope_type='random', options=None, data_path='./val2014', batch_size=1, beam=1, sample=False, use_attn=True, alpha=0.2, use_mask=False, use_cfg=False, gamma=1.1, start_layer=2, end_layer=32, max_tokens=512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model = \"llava-1.5\"\n",
    "args.pope_type = 'random'\n",
    "args.data_path = './val2014'\n",
    "args.gamma = 1.1\n",
    "args.use_attn = True\n",
    "args.use_cfg = False\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856d9dac-9acd-4730-8375-6164e1ce9de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use LN for projection:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "setup_seeds()\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "model_loader = ModelLoader(args.model)\n",
    "\n",
    "args.pope_path = POPE_CHAT_PATH[args.pope_type]\n",
    "pope_dataset = POPEChatDataSet(\n",
    "    pope_path=args.pope_path,\n",
    "    data_path=args.data_path,\n",
    "    trans=model_loader.image_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74669cee-19a7-4cec-aa99-4b56fbb9ab4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "    (vision_tower): CLIPVisionTower(\n",
       "      (vision_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(577, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_loader.vlm_model.to(device)\n",
    "model_loader.llm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28d2cc3-d300-48de-9a9d-4079148de627",
   "metadata": {},
   "outputs": [],
   "source": [
    "pope_loader = torch.utils.data.DataLoader(\n",
    "    pope_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "base_dir = \"./pope/\" + args.model\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "# dump metric file\n",
    "file_parts = [\n",
    "    f\"pope_eval_{args.pope_type}_layers_{args.start_layer}-{args.end_layer}_tokens_{args.max_tokens}_eos\",\n",
    "    \"_sample\" if args.sample else \"\",\n",
    "    f\"_beams_{args.beam}\" if args.beam != 1 else \"\",\n",
    "    f\"_attn_{args.alpha}\" if args.use_attn else \"\",\n",
    "    f\"_cfg_{args.gamma}\" if args.use_cfg else \"\",\n",
    "]\n",
    "\n",
    "file_name = \"\".join(file_parts)\n",
    "template = INSTRUCTION_TEMPLATE[args.model]\n",
    "if args.model == \"llava-1.5\" or args.model == \"shikra\":\n",
    "    template = SYSTEM_MESSAGE + template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202561a2-5a76-4535-9a3f-fde090546df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pope_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3cff8f-0396-4a3a-833f-abdd7f913606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [49:58<00:00,  6.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch_id, data in tqdm(enumerate(pope_loader), total=len(pope_loader)):\n",
    "    image = data[\"image\"]\n",
    "    queries = np.array(data[\"query\"])\n",
    "    label = torch.stack(data[\"label\"])\n",
    "    kwargs = {}\n",
    "\n",
    "    round = label.size()[0]\n",
    "    for idx in range(round):\n",
    "        query = queries[idx, :].tolist()\n",
    "        lal = label[idx, :].tolist()\n",
    "        # prepare inputs for model\n",
    "        questions, kwargs = model_loader.prepare_inputs_for_model(\n",
    "            template, query, image\n",
    "        )\n",
    "\n",
    "        # llama_modify(\n",
    "        #     model_loader.llm_model,\n",
    "        #     args.start_layer,\n",
    "        #     args.end_layer,\n",
    "        #     args.use_attn,\n",
    "        #     args.alpha,\n",
    "        #     args.use_cfg,\n",
    "        #     model_loader.img_start_idx,\n",
    "        #     model_loader.img_end_idx,\n",
    "        # )\n",
    "\n",
    "        logits_processor = (\n",
    "            model_loader.init_cfg_processor(questions, args.gamma, args.beam, args.start_layer, args.end_layer)\n",
    "            if args.use_cfg\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if logits_processor is not None:\n",
    "            kwargs[\"logits_processor\"] = LogitsProcessorList([logits_processor])\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model_loader.llm_model.generate(\n",
    "                do_sample=args.sample,\n",
    "                max_new_tokens=args.max_tokens,\n",
    "                use_cache=True,\n",
    "                num_beams=args.beam,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        output_text = model_loader.decode(outputs)\n",
    "\n",
    "        for i in range(len(output_text)):\n",
    "            with open(os.path.join(base_dir, file_name + \".jsonl\"), \"a\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"query\": query[i],\n",
    "                        \"label\": lal[i],\n",
    "                        \"ans\": output_text[i],\n",
    "                        \"question\": questions[i],\n",
    "                        \"file_path\": file_name,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17c49f5c-fa65-4ff8-af32-12a7002d16ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loader.llm_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e6ce5eb-1fa7-4368-a68f-6dbf7b98a28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': {'pixel_values': [tensor([[[[ 1.5216e+00,  1.5216e+00,  1.5216e+00,  ...,  1.7990e+00,\n",
       "             1.7990e+00,  1.7990e+00],\n",
       "           [ 1.5216e+00,  1.5216e+00,  1.5216e+00,  ...,  1.7990e+00,\n",
       "             1.7990e+00,  1.7990e+00],\n",
       "           [ 1.5216e+00,  1.5216e+00,  1.5216e+00,  ...,  1.7990e+00,\n",
       "             1.7990e+00,  1.7990e+00],\n",
       "           ...,\n",
       "           [-1.8644e-01, -2.1563e-01, -1.8644e-01,  ..., -8.5796e-01,\n",
       "            -8.4336e-01, -9.1636e-01],\n",
       "           [-4.0451e-02, -9.8845e-02, -1.4264e-01,  ..., -7.9957e-01,\n",
       "            -7.2658e-01, -7.9957e-01],\n",
       "           [-2.7403e-01, -1.1255e-02, -1.5724e-01,  ..., -5.9519e-01,\n",
       "            -6.9738e-01, -6.9738e-01]],\n",
       " \n",
       "          [[ 1.7297e+00,  1.7297e+00,  1.7297e+00,  ...,  1.9398e+00,\n",
       "             1.9398e+00,  1.9398e+00],\n",
       "           [ 1.7297e+00,  1.7297e+00,  1.7297e+00,  ...,  1.9398e+00,\n",
       "             1.9398e+00,  1.9398e+00],\n",
       "           [ 1.7297e+00,  1.7297e+00,  1.7297e+00,  ...,  1.9398e+00,\n",
       "             1.9398e+00,  1.9398e+00],\n",
       "           ...,\n",
       "           [-1.1625e-01, -1.1625e-01, -1.1625e-01,  ..., -7.9160e-01,\n",
       "            -7.7659e-01, -8.5163e-01],\n",
       "           [ 4.8835e-02,  3.8118e-03, -4.1212e-02,  ..., -7.3157e-01,\n",
       "            -6.5653e-01, -7.3157e-01],\n",
       "           [-1.9129e-01,  1.0887e-01, -4.1212e-02,  ..., -5.2146e-01,\n",
       "            -6.2651e-01, -6.2651e-01]],\n",
       " \n",
       "          [[ 1.8757e+00,  1.8757e+00,  1.8757e+00,  ...,  2.0179e+00,\n",
       "             2.0179e+00,  2.0179e+00],\n",
       "           [ 1.8757e+00,  1.8757e+00,  1.8757e+00,  ...,  2.0179e+00,\n",
       "             2.0179e+00,  2.0179e+00],\n",
       "           [ 1.8757e+00,  1.8757e+00,  1.8757e+00,  ...,  2.0179e+00,\n",
       "             2.0179e+00,  2.0179e+00],\n",
       "           ...,\n",
       "           [-1.5553e-02, -1.5553e-02, -1.3329e-03,  ..., -5.7014e-01,\n",
       "            -5.5592e-01, -6.2702e-01],\n",
       "           [ 1.4087e-01,  9.8208e-02,  5.5547e-02,  ..., -5.1326e-01,\n",
       "            -4.4215e-01, -5.1326e-01],\n",
       "           [-5.8213e-02,  1.9775e-01,  4.1327e-02,  ..., -3.1417e-01,\n",
       "            -4.1371e-01, -4.1371e-01]]]])]},\n",
       " 'query': [('Is there a person in the image?',),\n",
       "  ('Is there a teddy bear in the image?',),\n",
       "  ('Is there a potted plant in the image?',),\n",
       "  ('Is there a broccoli in the image?',),\n",
       "  ('Is there a car in the image?',),\n",
       "  ('Is there an orange in the image?',)],\n",
       " 'label': [tensor([1]),\n",
       "  tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([0])]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebcce5-e509-4797-b9a0-0830400cf6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2721c-818c-40c8-a847-8b763cc5ffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f520701-58c8-45e6-a93c-0e9a3e324357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12ec86-98c3-41dd-a260-deaf5bbba645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a264748-e857-4207-b1f2-30f2ffd17315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef540989-c863-42d0-b695-c8554bf06bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc947f-35e9-48e0-8e84-c3529c5890d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b50e40-1677-4b1b-8815-2590a8c13eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7dfcf-9544-4b63-9307-1aef55c6c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c71937-7d86-4849-a8d1-5e41fb0712e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
